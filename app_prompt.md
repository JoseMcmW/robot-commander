# Prompt: Voice + Vision Robot Commander

## ğŸ“‹ DescripciÃ³n General

Necesito una aplicaciÃ³n web que permite controlar un robot simulado usando comandos de voz en lenguaje natural combinados con reconocimiento de objetos en tiempo real mediante visiÃ³n por computadora. La aplicaciÃ³n debe implementar un sistema multi-agente de IA donde cada agente especializado procesa una parte del pipeline de control.

## ğŸ¯ Objetivo del Proyecto

Crear una demo interactiva de robÃ³tica educativa que demuestre la integraciÃ³n de:
- Machine Learning en el browser (TensorFlow.js)
- Procesamiento de lenguaje natural con LLMs
- Arquitectura multi-agente (patrÃ³n LangGraph)
- VisiÃ³n por computadora en tiempo real

**Contexto:** Es para una entrevista en Educabot (empresa de robÃ³tica educativa) para una posiciÃ³n de Frontend Developer con Ã©nfasis en IA.

## ğŸ—ï¸ Arquitectura del Sistema

### Sistema Multi-Agente (4 agentes especializados)

1. **Speech Agent** 
   - Procesa comandos de voz usando Gemini API
   - Extrae intenciÃ³n del usuario (acciÃ³n, objetivo, propiedades)
   - Input: TranscripciÃ³n de voz
   - Output: `{ action, target, properties }`

2. **Vision Agent**
   - Analiza objetos detectados por la cÃ¡mara
   - Determina si coinciden con lo que busca el usuario
   - Input: Array de detecciones + propiedades buscadas
   - Output: `{ found, bestMatch, confidence, position }`

3. **Planner Agent**
   - Planifica secuencia de acciones del robot
   - Considera: comando del usuario, anÃ¡lisis de visiÃ³n, estado del robot
   - Input: Resultados de Speech + Vision + Estado actual
   - Output: `{ actions[], reasoning }`

4. **Executor Agent**
   - Ejecuta las acciones planificadas en el robot
   - Controla movimientos y animaciones
   - Input: Array de acciones
   - Output: Movimientos visuales del robot

### Orchestrator
Coordina los 4 agentes en secuencia (patrÃ³n LangGraph):
```
User Command â†’ Speech â†’ Vision â†’ Planner â†’ Executor â†’ Result
```

## ğŸ› ï¸ Stack TecnolÃ³gico

### Frontend
- **React 18** con TypeScript
- **Vite** como build tool
- **Tailwind CSS** para estilos
- **Zustand** para state management global

### Machine Learning
- **TensorFlow.js** (runtime de ML en browser)
- **COCO-SSD** (modelo pre-entrenado para detecciÃ³n de 80 objetos)
- Procesamiento en cliente (sin servidor ML)

### Inteligencia Artificial
- **Gemini 1.5 Flash API** (Google) para procesamiento de lenguaje natural
- Multi-agent system inspirado en LangGraph
- Prompts estructurados para cada agente

### APIs del Browser
- **Web Speech API** para reconocimiento de voz nativo
- **MediaDevices API** para acceso a webcam
- **Canvas 2D** para renderizar robot y detecciones

### Deploy
- **Vercel** con CI/CD automÃ¡tico
- Variables de entorno para API keys

## ğŸ¨ Funcionalidades EspecÃ­ficas

### 1. DetecciÃ³n de Objetos en Tiempo Real
- Webcam captura video en 640x480
- TensorFlow.js + COCO-SSD detecta objetos en cada frame (~30 FPS)
- Canvas overlay dibuja bounding boxes sobre objetos detectados
- Muestra etiquetas con nombre y confianza (ej: "person (87%)")
- Lista de objetos detectados actualizada en tiempo real

### 2. Control por Voz
- BotÃ³n de micrÃ³fono activa/desactiva grabaciÃ³n
- Web Speech API transcribe audio a texto
- Soporta comandos en espaÃ±ol e inglÃ©s
- Ejemplos de comandos:
  - "Robot, muÃ©vete hacia adelante"
  - "Gira a la izquierda"
  - "Ve hacia la persona"
  - "Busca el objeto rojo"

### 3. Robot Simulado
- Canvas 2D (600x600px) con grid de fondo
- Robot representado como rectÃ¡ngulo con indicador de direcciÃ³n
- Estado: posiciÃ³n (x, y), rotaciÃ³n (grados)
- Acciones disponibles:
  - `move_forward` / `move_backward` (con steps)
  - `turn_left` / `turn_right` (con grados)
  - `stop`
- Animaciones suaves entre movimientos

### 4. Dashboard de Estado
- Estado actual del sistema (procesando, esperando, error)
- Indicadores visuales de cada agente activo
- Historial de comandos ejecutados (Ãºltimos 10)
- ExplicaciÃ³n de la Ãºltima decisiÃ³n de la IA
- Contador de objetos detectados

### 5. Modo Demo
- BotÃ³n "Probar comando rÃ¡pido" ejecuta comando predefinido
- Funciona sin API key (respuestas simuladas)
- Ãštil para testing y presentaciones

## ğŸ“ Estructura de CÃ³digo

### Componentes React

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ WebcamCapture.tsx       # CÃ¡mara + detecciÃ³n ML
â”‚   â”œâ”€â”€ VoiceInput.tsx          # Control de micrÃ³fono
â”‚   â”œâ”€â”€ RobotCanvas.tsx         # SimulaciÃ³n del robot
â”‚   â”œâ”€â”€ AgentDashboard.tsx      # Estado de agentes (opcional)
â”‚   â””â”€â”€ ControlPanel.tsx        # Controles manuales (opcional)
```

### Sistema de Agentes (Arquitectura Modular)

```
src/agents/
â”œâ”€â”€ geminiClient.ts       # Cliente base de Gemini API
â”œâ”€â”€ speechAgent.ts        # Agente 1: Procesa voz
â”œâ”€â”€ visionAgent.ts        # Agente 2: Analiza visiÃ³n
â”œâ”€â”€ plannerAgent.ts       # Agente 3: Planifica acciones
â”œâ”€â”€ executorAgent.ts      # Agente 4: Ejecuta movimientos
â”œâ”€â”€ orchestrator.ts       # Coordinador de agentes
â””â”€â”€ index.ts              # Barrel export
```

### Machine Learning

```
src/ml/
â”œâ”€â”€ objectDetection.ts    # Wrapper de COCO-SSD
â””â”€â”€ colorDetection.ts     # DetecciÃ³n de colores (opcional)
```

### State Management

```
src/store/
â””â”€â”€ useRobotStore.ts      # Zustand store para estado del robot
```

## ğŸ”„ Flujo de EjecuciÃ³n Detallado

### 1. InicializaciÃ³n
```
- App carga
- TensorFlow.js inicializa backend (WebGL o CPU)
- COCO-SSD descarga modelo (~5MB)
- Webcam solicita permisos y se activa
- Robot aparece en centro del canvas
```

### 2. DetecciÃ³n Continua
```
Loop infinito:
- Capturar frame de video
- COCO-SSD.detect(frame) â†’ predictions[]
- Dibujar video + bounding boxes en canvas
- Actualizar lista de objetos detectados
- requestAnimationFrame â†’ repeat
```

### 3. Comando de Voz
```
Usuario habla â†’ Web Speech API transcribe â†’ onCommand(text)
â”‚
â””â”€> Orchestrator.orchestrateCommand(text, detections, robotState)
    â”‚
    â”œâ”€> Speech Agent: callGemini(prompt) 
    â”‚   â””â”€> Parse JSON â†’ { action, target, properties }
    â”‚
    â”œâ”€> Vision Agent: callGemini(prompt)
    â”‚   â””â”€> Analiza detecciones â†’ { found, bestMatch, confidence }
    â”‚
    â”œâ”€> Planner Agent: callGemini(prompt)
    â”‚   â””â”€> Genera plan â†’ { actions[], reasoning }
    â”‚
    â””â”€> Executor Agent: executeActions(actions[])
        â””â”€> Para cada acciÃ³n:
            - store.moveForward() / turnLeft() / etc.
            - await delay(200ms) para animaciÃ³n
            - Actualizar canvas
```

### 4. Renderizado del Robot
```
useEffect en RobotCanvas:
- Leer estado: { x, y, rotation, currentAction }
- Limpiar canvas
- Dibujar grid de fondo
- ctx.save()
- ctx.translate(x, y)
- ctx.rotate(rotation)
- Dibujar cuerpo del robot (rectÃ¡ngulo)
- Dibujar indicador de direcciÃ³n (triÃ¡ngulo)
- ctx.restore()
- Dibujar texto con acciÃ³n actual
```

## ğŸ¨ DiseÃ±o UI/UX

### Layout
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– Voice + Vision Robot Commander              â”‚
â”‚  Controla el robot usando voz + IA              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚    â”‚
â”‚  â”‚   Webcam Feed    â”‚  â”‚  Robot Canvas    â”‚    â”‚
â”‚  â”‚   + Detections   â”‚  â”‚   (SimulaciÃ³n)   â”‚    â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ğŸ¤ Voice Control â”‚  â”‚ ğŸ“Š Estado        â”‚    â”‚
â”‚  â”‚  [MicrÃ³fono]    â”‚  â”‚  Sistema activo  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Objetos: 3       â”‚  â”‚ ğŸ“ Historial     â”‚    â”‚
â”‚  â”‚ person, tv, cup  â”‚  â”‚  1. "muÃ©vete..." â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Paleta de Colores
- Fondo: Gradiente oscuro (gray-950 â†’ blue-950)
- Primario: Azul (#3b82f6)
- Ã‰xito: Verde (#10b981)
- Error: Rojo (#ef4444)
- Texto: Blanco/Gray-300

### Animaciones
- Pulse en botÃ³n de micrÃ³fono cuando estÃ¡ grabando
- Spinner de loading al procesar comandos
- Smooth transitions en movimientos del robot
- Fade in/out de mensajes de estado

## ğŸ” ConfiguraciÃ³n

### Variables de Entorno
```bash
# .env.local
VITE_GEMINI_API_KEY=tu_api_key_de_google_gemini
```

### Obtener API Key
1. Ir a https://aistudio.google.com/app/apikey
2. Crear API key (gratis: 60 req/min, 1500/dÃ­a)
3. Copiar a `.env.local`

### Fallback sin API Key
Si no hay API key, el sistema usa respuestas simuladas (modo demo).

## ğŸ“Š Tipos TypeScript

### Interfaces Principales

```typescript
// Speech Agent
interface SpeechResult {
  action: 'move_to' | 'turn' | 'search' | 'stop';
  target: string;
  properties: Record<string, any>;
}

// Vision Agent
interface VisionResult {
  found: boolean;
  bestMatch: string | null;
  confidence: number;
  position: 'left' | 'center' | 'right';
}

// Planner Agent
interface RobotAction {
  type: 'move_forward' | 'move_backward' | 'turn_left' | 'turn_right' | 'stop';
  steps?: number;
  amount?: number;
}

interface ActionPlan {
  actions: RobotAction[];
  reasoning: string;
}

// Robot State
interface RobotState {
  x: number;
  y: number;
  rotation: number;
  isMoving: boolean;
  currentAction: string;
  commandHistory: string[];
}
```

## ğŸ§ª Testing / Demo

### Escenarios de Prueba

1. **DetecciÃ³n bÃ¡sica**
   - Mostrar objetos a la cÃ¡mara
   - Verificar detecciÃ³n y etiquetado correcto

2. **Comando simple**
   - "MuÃ©vete hacia adelante"
   - Robot debe avanzar 3 pasos

3. **Comando con direcciÃ³n**
   - "Gira a la derecha"
   - Robot debe rotar 45Â°

4. **Comando con objeto detectado**
   - Detectar "person" en cÃ¡mara
   - Decir "ve hacia la persona"
   - Robot debe avanzar

5. **Comando complejo**
   - "Busca el objeto rojo y gira"
   - MÃºltiples acciones en secuencia

## âš ï¸ Consideraciones Importantes

### Performance
- TensorFlow.js puede ser lento en CPU (aceptable para demo)
- WebGL acelera significativamente si estÃ¡ disponible
- COCO-SSD es ligero (~6MB) pero toma 2-3s en cargar

### Limitaciones
- COCO-SSD solo detecta 80 clases predefinidas
- No detecta colores especÃ­ficos (requiere modelo custom)
- Gemini API tiene rate limits (60 req/min gratis)
- Web Speech API solo funciona en Chrome/Edge

### Error Handling
- Fallbacks automÃ¡ticos si Gemini falla
- Modo demo si no hay API key
- Mensajes claros de error al usuario
- Logging extensivo en consola para debugging

## ğŸ“š DocumentaciÃ³n Adicional

### Prompts para Gemini

**Speech Agent Prompt:**
```
Analiza este comando de voz: "${transcript}"
Extrae: action, target, properties
Responde SOLO con JSON vÃ¡lido sin markdown.
```

**Vision Agent Prompt:**
```
Objetos detectados: ${detections}
Usuario busca: ${targetProperties}
Â¿Alguno coincide?
Responde SOLO con JSON: { found, bestMatch, confidence, position }
```

**Planner Agent Prompt:**
```
Comando: ${speechResult}
VisiÃ³n: ${visionResult}
Estado robot: posiciÃ³n (${x}, ${y}), rotaciÃ³n ${rotation}Â°
Planea secuencia de acciones.
Responde SOLO con JSON: { actions[], reasoning }
```

### Comandos npm

```bash
# Desarrollo
npm run dev

# Build
npm run build

# Preview build
npm run preview

# Type check
npx tsc --noEmit

# Deploy
vercel --prod
```

## ğŸ¯ Entregables

1. **CÃ³digo fuente**
   - Repositorio con estructura modular
   - TypeScript con tipos completos
   - Comentarios en cÃ³digo clave

2. **Demo funcionando**
   - URL pÃºblica en Vercel
   - Funcional con API key configurada

3. **DocumentaciÃ³n**
   - README.md con instrucciones
   - Diagramas de arquitectura
   - ExplicaciÃ³n de decisiones tÃ©cnicas

4. **PresentaciÃ³n**
   - Demo en vivo de 3-5 minutos
   - ExplicaciÃ³n del sistema multi-agente
   - CÃ³digo destacando partes clave

## ğŸ’¡ Puntos a Destacar en Entrevista

1. **Arquitectura moderna**: Multi-agente con separaciÃ³n de concerns
2. **ML en browser**: Sin backend, todo en cliente
3. **IntegraciÃ³n de LLMs**: Uso prÃ¡ctico de IA generativa
4. **Clean Code**: Estructura modular, TypeScript, buenas prÃ¡cticas
5. **Alineado con Educabot**: RobÃ³tica educativa + IA accesible

---

## âœ… Requisitos CrÃ­ticos

- [ ] Frontend en React + TypeScript + Vite
- [ ] TensorFlow.js con COCO-SSD funcionando
- [ ] Gemini API integrada (o cualquier LLM)
- [ ] Sistema multi-agente (4 agentes mÃ­nimo)
- [ ] Webcam con detecciÃ³n en tiempo real
- [ ] Robot simulado con movimientos animados
- [ ] Deploy en Vercel
- [ ] Arquitectura modular y escalable
- [ ] Error handling robusto
- [ ] UI responsive y moderna

---

**Este proyecto debe demostrar conocimientos de Frontend + IA + ML en un contexto de robÃ³tica educativa.**